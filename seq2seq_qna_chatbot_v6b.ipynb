{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53310f11",
   "metadata": {},
   "source": [
    "# ğŸ‡°ğŸ‡· Seq2Seq Q&A Chatbot (Korean, SentencePiece) â€” í”„ë¡œì íŠ¸ ë…¸íŠ¸ë¶\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **í•œêµ­ì–´ Q&A Chatbot**ì„ ìœ„í•œ **Seq2Seq (Encoderâ€“Decoder, Teacher Forcing)** íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ì˜ˆì‹œì…ë‹ˆë‹¤.  \n",
    "ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. **ë°ì´í„° ì·¨ë“**: ê³µê°œ í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°ì…‹(`ChatbotData.csv`) ë¡œë“œ  \n",
    "2. **ë°ì´í„° ì „ì²˜ë¦¬**: ì •ë¦¬, ì •ê·œí™”, ë¶„ë¦¬  \n",
    "3. **í† í¬ë‚˜ì´ì € í•™ìŠµ (SentencePiece)**  \n",
    "   - `<bos>, <eos>, <pad>, <oov>` ì§€ì •  \n",
    "   - `set_encode_extra_options(':')`, `bos:`, `:eos`, `bos:eos` ì‚¬ìš© ì˜ˆì‹œ  \n",
    "4. **í•™ìŠµìš© í…ì„œ ìƒì„±**: `Q_input`, `A_input`, `A_target`  \n",
    "5. **ëª¨ë¸ ìƒì„±**: Encoder, Decoder(Teacher Forcing)  \n",
    "6. **í•™ìŠµ**  \n",
    "7. **ëª¨ë¸ ì¶”ë¡ **: Inferenceìš© Decoder, Greedy Decode í•¨ìˆ˜  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad579b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, random\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "SEED=42; np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "DATA_URL=\"https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv\"\n",
    "DATA_DIR=Path(\"./data\"); DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RAW_CSV=DATA_DIR/\"ChatbotData.csv\"\n",
    "SPM_PREF=str(DATA_DIR/\"spm_kor_v6b\")\n",
    "\n",
    "VOCAB_SIZE=4000\n",
    "EMB_DIM=256; HID_DIM=384\n",
    "MAX_LEN_Q=28; MAX_LEN_A=28\n",
    "BATCH_SIZE=32; EPOCHS=25\n",
    "DROPOUT=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13df9a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: (11750, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12ì‹œ ë•¡!</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q            A\n",
       "0        12ì‹œ ë•¡!   í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.\n",
       "1   1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´    ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.\n",
       "2  3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ ."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1) ë°ì´í„°\n",
    "df=pd.read_csv(DATA_URL)[['Q','A']].dropna()\n",
    "df['Q']=df['Q'].astype(str).str.strip(); df['A']=df['A'].astype(str).str.strip()\n",
    "df=df[(df['Q']!='')&(df['A']!='')].drop_duplicates().reset_index(drop=True)\n",
    "df.to_csv(RAW_CSV, index=False, encoding='utf-8')\n",
    "print(\"Raw:\", df.shape); df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9159cfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10575, 1175)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 2) ì „ì²˜ë¦¬\n",
    "def norm_ko(t):\n",
    "    t=str(t)\n",
    "    t=re.sub(r\"[\\t]+\",\" \",t)\n",
    "    t=re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "df['Q_norm']=df['Q'].apply(norm_ko)\n",
    "df['A_norm']=df['A'].apply(norm_ko)\n",
    "df=df[(df['Q_norm'].str.len()>0)&(df['A_norm'].str.len()>0)]\n",
    "train_df, val_df = train_test_split(df[['Q_norm','A_norm']], test_size=0.1, random_state=SEED, shuffle=True)\n",
    "len(train_df), len(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c7dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=data/spm_corpus_v6b.txt --model_prefix=data/spm_kor_v6b --vocab_size=4000 --model_type=unigram --character_coverage=0.9995 --max_sentence_length=999999 --pad_id=0 --pad_piece=<pad> --bos_id=1 --bos_piece=<bos> --eos_id=2 --eos_piece=<eos> --unk_id=3 --unk_piece=<oov>\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/spm_corpus_v6b.txt\n",
      "  input_format: \n",
      "  model_prefix: data/spm_kor_v6b\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <oov>\n",
      "  bos_piece: <bos>\n",
      "  eos_piece: <eos>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: data/spm_corpus_v6b.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 21150 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <bos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <oov>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=316467\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1074\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 21150 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=139224\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 17655 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 21150\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 20462\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20462 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10248 obj=13.2536 num_tokens=43683 num_tokens/piece=4.26259\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9059 obj=12.1578 num_tokens=43815 num_tokens/piece=4.83663\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6793 obj=12.5279 num_tokens=46678 num_tokens/piece=6.87149\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6792 obj=12.4525 num_tokens=46704 num_tokens/piece=6.87633\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5094 obj=12.9972 num_tokens=50582 num_tokens/piece=9.92972\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5094 obj=12.912 num_tokens=50649 num_tokens/piece=9.94287\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4400 obj=13.3525 num_tokens=52767 num_tokens/piece=11.9925\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4400 obj=13.3054 num_tokens=52776 num_tokens/piece=11.9945\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: data/spm_kor_v6b.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: data/spm_kor_v6b.vocab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) SentencePiece (4k vocab)\n",
    "corpus_path=DATA_DIR/\"spm_corpus_v6b.txt\"\n",
    "with open(corpus_path,\"w\",encoding=\"utf-8\") as f:\n",
    "    for s in pd.concat([train_df['Q_norm'], train_df['A_norm']], axis=0):\n",
    "        f.write(s+\"\\n\")\n",
    "\n",
    "spm_cmd=(f\"--input={corpus_path} --model_prefix={SPM_PREF} --vocab_size={VOCAB_SIZE} \"\n",
    "         f\"--model_type=unigram --character_coverage=0.9995 \"\n",
    "         f\"--max_sentence_length=999999 \"\n",
    "         f\"--pad_id=0 --pad_piece=<pad> --bos_id=1 --bos_piece=<bos> \"\n",
    "         f\"--eos_id=2 --eos_piece=<eos> --unk_id=3 --unk_piece=<oov>\")\n",
    "spm.SentencePieceTrainer.train(spm_cmd)\n",
    "sp=spm.SentencePieceProcessor(model_file=str(Path(SPM_PREF + \".model\")))\n",
    "print(\"Vocab:\", sp.vocab_size())\n",
    "\n",
    "def sp_encode_ids(text, extra=':'):\n",
    "    sp.set_encode_extra_options(extra)\n",
    "    ids=sp.encode(str(text), out_type=int)\n",
    "    sp.set_encode_extra_options('')\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a12bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10575, 28), (10575, 28), (10575, 28), (10575, 28))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 4) í…ì„œ + PAD ë§ˆìŠ¤í‚¹\n",
    "PAD_ID,BOS_ID,EOS_ID,OOV_ID=0,1,2,3\n",
    "VOCAB_SIZE=sp.vocab_size()\n",
    "\n",
    "def encode_pair(q,a):\n",
    "    return sp_encode_ids(q, \":\"), sp_encode_ids(a, \"bos:\"), sp_encode_ids(a, \":eos\")\n",
    "\n",
    "def pad_to(ids, L): return ids[:L] if len(ids)>=L else ids+[PAD_ID]*(L-len(ids))\n",
    "\n",
    "def build_tensors(frame):\n",
    "    Qs,Ains,Atgts=[],[],[]\n",
    "    for q,a in zip(frame['Q_norm'], frame['A_norm']):\n",
    "        q_ids,a_in,a_tgt=encode_pair(q,a)\n",
    "        Qs.append(pad_to(q_ids,MAX_LEN_Q))\n",
    "        Ains.append(pad_to(a_in,MAX_LEN_A))\n",
    "        Atgts.append(pad_to(a_tgt,MAX_LEN_A))\n",
    "    Xq=np.array(Qs,np.int32); Xin=np.array(Ains,np.int32); Y=np.array(Atgts,np.int32)\n",
    "    W=(Y!=PAD_ID).astype(\"float32\")  # PAD ë¬´ì‹œ\n",
    "    return Xq,Xin,Y,W\n",
    "\n",
    "X_enc_tr,X_dec_in_tr,Y_tr,W_tr=build_tensors(train_df)\n",
    "X_enc_va,X_dec_in_va,Y_va,W_va=build_tensors(val_df)\n",
    "\n",
    "X_enc_tr.shape, X_dec_in_tr.shape, Y_tr.shape, W_tr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694988a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 03:05:44.215133: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-08-27 03:05:44.215163: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-08-27 03:05:44.215180: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-08-27 03:05:44.215197: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-27 03:05:44.215209: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'lambda' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'lambda_3' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'lambda_5' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_v6b\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2seq_v6b\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dec_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ enc_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ tok_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> â”‚ enc_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ dec_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ tok_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ enc_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bilstm_enc          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>) â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">689,664</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚                   â”‚            â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>) â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> â”‚ bilstm_enc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ tok_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,840</span> â”‚ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,840</span> â”‚ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal_2         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ enc_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dec_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)     â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">984,576</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)]             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>) â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> â”‚ dec_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             â”‚            â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             â”‚            â”‚ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ out_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076,000</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)             â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dec_in (\u001b[38;5;33mInputLayer\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ enc_in (\u001b[38;5;33mInputLayer\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ tok_emb (\u001b[38;5;33mEmbedding\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â”‚  \u001b[38;5;34m1,024,000\u001b[0m â”‚ enc_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ dec_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ tok_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ enc_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”‚ (\u001b[38;5;33mNotEqual\u001b[0m)          â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bilstm_enc          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m) â”‚    \u001b[38;5;34m689,664\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚                   â”‚            â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m) â”‚        \u001b[38;5;34m768\u001b[0m â”‚ bilstm_enc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (\u001b[38;5;33mLambda\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ tok_emb[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       â”‚    \u001b[38;5;34m147,840\u001b[0m â”‚ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       â”‚    \u001b[38;5;34m147,840\u001b[0m â”‚ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal_2         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ enc_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”‚ (\u001b[38;5;33mNotEqual\u001b[0m)          â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dec_lstm (\u001b[38;5;33mLSTM\u001b[0m)     â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     â”‚    \u001b[38;5;34m984,576\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m384\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m384\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m384\u001b[0m)]             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m) â”‚        \u001b[38;5;34m768\u001b[0m â”‚ dec_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;45mNone\u001b[0m)             â”‚            â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;45mNone\u001b[0m)             â”‚            â”‚ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;45mNone\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_5 (\u001b[38;5;33mLambda\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ out_dense (\u001b[38;5;33mDense\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â”‚  \u001b[38;5;34m3,076,000\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m4000\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,071,456</span> (23.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,071,456\u001b[0m (23.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,071,456</span> (23.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,071,456\u001b[0m (23.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 5) ëª¨ë¸ â€” BiLSTM Encoder + Custom Luong (No custom Dense)\n",
    "embedding=layers.Embedding(VOCAB_SIZE, EMB_DIM, mask_zero=True, name=\"tok_emb\")\n",
    "\n",
    "enc_inputs=layers.Input(shape=(None,), name=\"enc_in\")\n",
    "dec_inputs=layers.Input(shape=(None,), name=\"dec_in\")\n",
    "\n",
    "# Encoder\n",
    "enc_emb=embedding(enc_inputs)\n",
    "enc_emb=layers.Dropout(DROPOUT)(enc_emb)\n",
    "enc_bi=layers.Bidirectional(layers.LSTM(HID_DIM//2, return_sequences=True, name=\"enc_lstm\"), name=\"bilstm_enc\")(enc_emb)\n",
    "enc_bi=layers.LayerNormalization()(enc_bi)\n",
    "\n",
    "# Decoder\n",
    "dec_emb=embedding(dec_inputs)\n",
    "dec_emb=layers.Dropout(DROPOUT)(dec_emb)\n",
    "dec_lstm=layers.LSTM(HID_DIM, return_sequences=True, return_state=True, name=\"dec_lstm\")\n",
    "enc_mean = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(enc_bi)\n",
    "init_h = layers.Dense(HID_DIM, activation=\"tanh\")(enc_mean)\n",
    "init_c = layers.Dense(HID_DIM, activation=\"tanh\")(enc_mean)\n",
    "dec_out, _, _ = dec_lstm(dec_emb, initial_state=[init_h, init_c])\n",
    "dec_out=layers.LayerNormalization()(dec_out)\n",
    "\n",
    "# Mask\n",
    "enc_mask_bool=embedding.compute_mask(enc_inputs)\n",
    "enc_mask=layers.Lambda(lambda m: tf.cast(m, tf.float32))(enc_mask_bool)\n",
    "enc_mask_exp=layers.Lambda(lambda m: tf.expand_dims(m,1))(enc_mask)\n",
    "\n",
    "# Luong\n",
    "score=layers.Lambda(lambda xy: tf.matmul(xy[0], tf.transpose(xy[1],perm=[0,2,1])))([dec_out, enc_bi])\n",
    "minus_inf=layers.Lambda(lambda m: (1.0-m)*-1e9)(enc_mask_exp)\n",
    "score=layers.Add()([score, minus_inf])\n",
    "attn_w=layers.Activation(\"softmax\")(score)\n",
    "context=layers.Lambda(lambda xy: tf.matmul(xy[0], xy[1]))([attn_w, enc_bi])\n",
    "dec_cat=layers.Concatenate()([dec_out, context])\n",
    "\n",
    "# Standard output Dense (no tied weights)\n",
    "logits=layers.Dense(VOCAB_SIZE, activation=None, name=\"out_dense\")(dec_cat)\n",
    "\n",
    "model=models.Model([enc_inputs, dec_inputs], logits, name=\"seq2seq_v6b\")\n",
    "model.summary()\n",
    "\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt=optimizers.Adam(learning_rate=3e-4, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8485ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 03:05:45.950802: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 137ms/step - accuracy: 0.0714 - loss: 1.6733 - val_accuracy: 0.0846 - val_loss: 1.5326 - learning_rate: 3.0000e-04\n",
      "Epoch 2/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 129ms/step - accuracy: 0.0106 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 3.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 130ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 3.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 128ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.5000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 128ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.5000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 128ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 7.5000e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) í•™ìŠµ\n",
    "ckpt=Path(\"./checkpoints_v6b\"); ckpt.mkdir(exist_ok=True)\n",
    "cbs=[\n",
    "    callbacks.ModelCheckpoint(str(ckpt/\"weights.keras\"), save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "history=model.fit([X_enc_tr, X_dec_in_tr], Y_tr, sample_weight=W_tr,\n",
    "                  validation_data=([X_enc_va, X_dec_in_va], Y_va, W_va),\n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185aa1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'lambda_6' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ì•ˆë…•? \n",
      "A(beam): ì˜.\n",
      "Q: ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ? \n",
      "A(beam): ì˜. ê±°ì˜ˆìš”\n",
      "Q: ì·¨ë¯¸ê°€ ë­ì•¼? \n",
      "A(beam): ì˜. ê±°ì˜ˆìš”.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) ì¶”ë¡ /ì±„íŒ… (ë™ì¼)\n",
    "encoder_model=models.Model(enc_inputs, [enc_bi, init_h, init_c], name=\"enc_infer\")\n",
    "\n",
    "dec_state_h=layers.Input(shape=(HID_DIM,), name=\"dec_state_h\")\n",
    "dec_state_c=layers.Input(shape=(HID_DIM,), name=\"dec_state_c\")\n",
    "enc_seq_in=layers.Input(shape=(None,HID_DIM), name=\"enc_seq_in\")\n",
    "dec_tok_in=layers.Input(shape=(1,), name=\"dec_tok_in\")\n",
    "\n",
    "dec_tok_emb=embedding(dec_tok_in)\n",
    "dec_step, n_h, n_c = dec_lstm(dec_tok_emb, initial_state=[dec_state_h, dec_state_c])\n",
    "\n",
    "score_step = layers.Lambda(lambda xy: tf.matmul(xy[0], tf.transpose(xy[1], perm=[0,2,1])))([dec_step, enc_seq_in])\n",
    "attn_w_step = layers.Activation(\"softmax\")(score_step)\n",
    "context_step = layers.Lambda(lambda xy: tf.matmul(xy[0], xy[1]))([attn_w_step, enc_seq_in])\n",
    "dec_cat_step = layers.Concatenate()([dec_step, context_step])\n",
    "logits_step = model.get_layer(\"out_dense\")(dec_cat_step)\n",
    "\n",
    "decoder_model=models.Model([dec_tok_in, dec_state_h, dec_state_c, enc_seq_in],\n",
    "                           [logits_step, n_h, n_c], name=\"dec_step\")\n",
    "\n",
    "def _norm(t): return re.sub(r\"[ \\t]+\",\" \",str(t)).strip()\n",
    "def _prep_q(q):\n",
    "    q_ids=sp_encode_ids(_norm(q), \":\")\n",
    "    q_ids=q_ids[:MAX_LEN_Q]+[PAD_ID]*(MAX_LEN_Q-len(q_ids))\n",
    "    arr=np.array(q_ids, np.int32)[None,:]\n",
    "    enc_seq, h, c = encoder_model.predict(arr, verbose=0)\n",
    "    return enc_seq, h, c\n",
    "\n",
    "def _ban_basic(logits):\n",
    "    logits[PAD_ID]=-1e9; logits[BOS_ID]=-1e9\n",
    "def _ban_repeating_ngrams(logits, ids, n=3):\n",
    "    if n<=1 or len(ids)<n-1: return\n",
    "    grams=set(tuple(ids[i:i+n]) for i in range(len(ids)-n+1))\n",
    "    prefix=tuple(ids[-(n-1):])\n",
    "    V=logits.shape[-1]\n",
    "    for v in range(V):\n",
    "        if prefix+(v,) in grams: logits[v]=-1e9\n",
    "def _cleanup(text: str) -> str:\n",
    "    text=re.sub(r\"(\\b\\S{1,6})( \\1){2,}\", r\"\\1 \\1\", text)\n",
    "    text=re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\", text)\n",
    "    text=re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def greedy_decode(q, max_len=28, no_repeat_ngram=3):\n",
    "    enc_seq,h,c=_prep_q(q); cur=BOS_ID; out=[]\n",
    "    for _ in range(max_len):\n",
    "        token=np.array([[cur]], np.int32)\n",
    "        logits,h,c=decoder_model.predict([token,h,c,enc_seq], verbose=0)\n",
    "        logits=logits[0,-1,:]\n",
    "        _ban_basic(logits); _ban_repeating_ngrams(logits, out, n=no_repeat_ngram)\n",
    "        nid=int(np.argmax(logits))\n",
    "        if nid in (EOS_ID, PAD_ID): break\n",
    "        out.append(nid); cur=nid\n",
    "    return _cleanup(sp.decode(out))\n",
    "\n",
    "def sampling_decode(q, max_len=28, top_p=0.9, temperature=0.9, no_repeat_ngram=3):\n",
    "    enc_seq,h,c=_prep_q(q); cur=BOS_ID; out=[]\n",
    "    for _ in range(max_len):\n",
    "        token=np.array([[cur]], np.int32)\n",
    "        logits,h,c=decoder_model.predict([token,h,c,enc_seq], verbose=0)\n",
    "        logits=logits[0,-1,:]/max(1e-6,temperature)\n",
    "        _ban_basic(logits); _ban_repeating_ngrams(logits, out, n=no_repeat_ngram)\n",
    "        probs=tf.nn.softmax(tf.convert_to_tensor(logits)).numpy()\n",
    "        idx=np.argsort(-probs); probs=probs[idx]; cum=probs.cumsum()\n",
    "        cut=np.searchsorted(cum, top_p)+1; idx=idx[:cut]; probs=probs[:cut]; probs=probs/probs.sum()\n",
    "        nid=int(np.random.choice(idx, p=probs))\n",
    "        if nid in (EOS_ID, PAD_ID): break\n",
    "        out.append(nid); cur=nid\n",
    "    return _cleanup(sp.decode(out))\n",
    "\n",
    "def beam_search_decode(q, max_len=28, beam_size=5, length_norm_alpha=0.8,\n",
    "                       repetition_penalty=1.15, no_repeat_ngram=3):\n",
    "    enc_seq,h0,c0=_prep_q(q)\n",
    "    beams=[(0.0,[BOS_ID],h0,c0)]; completed=[]\n",
    "    for _ in range(max_len):\n",
    "        new=[]\n",
    "        for lp,ids,h,c in beams:\n",
    "            cur=ids[-1]\n",
    "            if cur==EOS_ID: completed.append((lp,ids)); continue\n",
    "            token=np.array([[cur]], np.int32)\n",
    "            logits,nh,nc=decoder_model.predict([token,h,c,enc_seq], verbose=0)\n",
    "            logits=logits[0,-1,:]\n",
    "            _ban_basic(logits); _ban_repeating_ngrams(logits, ids[1:], n=no_repeat_ngram)\n",
    "            if repetition_penalty>1.0:\n",
    "                for v in set(ids): logits[v]=logits[v]/repetition_penalty\n",
    "            topk=int(beam_size*2)\n",
    "            cand=np.argpartition(-logits, topk)[:topk]\n",
    "            logp=tf.nn.log_softmax(tf.convert_to_tensor(logits)).numpy()\n",
    "            for v in cand:\n",
    "                new.append((lp+float(logp[v]), ids+[int(v)], nh, nc))\n",
    "        if not new: break\n",
    "        new.sort(key=lambda x:x[0], reverse=True)\n",
    "        beams=new[:beam_size]\n",
    "    completed.extend([(lp,ids) for lp,ids,_,_ in beams])\n",
    "    def score(lp, ids):\n",
    "        L=max(1,len([i for i in ids if i not in (BOS_ID, PAD_ID)]))\n",
    "        return lp/(((5+L)**length_norm_alpha)/(6**length_norm_alpha))\n",
    "    best=max(completed, key=lambda x: score(x[0], x[1]))\n",
    "    out=[i for i in best[1] if i not in (BOS_ID, EOS_ID, PAD_ID)]\n",
    "    return _cleanup(sp.decode(out))\n",
    "\n",
    "# quick sanity (í•™ìŠµ í›„ ì‹¤í–‰ ê¶Œì¥)\n",
    "for q in [\"ì•ˆë…•?\", \"ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?\", \"ì·¨ë¯¸ê°€ ë­ì•¼?\"]:\n",
    "    try:\n",
    "        print(\"Q:\", q, \"\\nA(beam):\", beam_search_decode(q))\n",
    "    except Exception as e:\n",
    "        print(\"Decode error (í•™ìŠµ ì „):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c01754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8) í‰ê°€ & ì½˜ì†”í˜• ì±„íŒ…\n",
    "def preview_predictions(n=10, method=\"beam\"):\n",
    "    idx=np.random.choice(len(val_df), size=min(n,len(val_df)), replace=False)\n",
    "    for i in idx:\n",
    "        q=val_df.iloc[i]['Q_norm']; ref=val_df.iloc[i]['A_norm']\n",
    "        pred=beam_search_decode(q) if method==\"beam\" else greedy_decode(q)\n",
    "        print(f\"Q: {q}\\nA*: {ref}\\nÃ‚ : {pred}\\n\"+\"-\"*80)\n",
    "\n",
    "def chat_cli(mode=\"beam\"):\n",
    "    print(\"ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "    while True:\n",
    "        try:\n",
    "            user=input(\"ë‚˜: \").strip()\n",
    "        except EOFError:\n",
    "            print(\"\\nì…ë ¥ ì¢…ë£Œ\"); break\n",
    "        if not user: \n",
    "            continue\n",
    "        if user==\"ì¢…ë£Œ\":\n",
    "            print(\"ì±—ë´‡: ì•ˆë…•!\"); break\n",
    "        try:\n",
    "            if mode==\"beam\":\n",
    "                ans=beam_search_decode(user, max_len=MAX_LEN_A)\n",
    "            elif mode==\"sample\":\n",
    "                ans=sampling_decode(user, max_len=MAX_LEN_A)\n",
    "            else:\n",
    "                ans=greedy_decode(user, max_len=MAX_LEN_A)\n",
    "        except Exception as e:\n",
    "            ans=f\"(ë””ì½”ë”© ì˜¤ë¥˜: {e})\"\n",
    "        print(\"ì±—ë´‡:\", ans)\n",
    "\n",
    "# preview_predictions(10, method=\"beam\")\n",
    "# chat_cli(\"beam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "891c0168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
      "ì±—ë´‡: ì˜.\n",
      "ì±—ë´‡: ì˜. ê±°ì˜ˆìš”.\n",
      "ì±—ë´‡: . ê²Œ ê±°ì˜ˆìš”\n",
      "ì±—ë´‡: ì•ˆë…•!\n"
     ]
    }
   ],
   "source": [
    "# ë¯¸ë¦¬ ìœ„ ì…€(ë””ì½”ë” ì •ì˜ ë“±)ì„ ëª¨ë‘ ì‹¤í–‰í•œ ë’¤ì—!\n",
    "chat_cli(\"beam\")   # ì½˜ì†” ì…ë ¥ ëŒ€í™” ì‹œì‘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
